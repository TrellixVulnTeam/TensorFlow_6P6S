# Module tensorflow/&zwnj;talkheads_ggelu_bert_en_large/1
BERT with Talking-Heads Attention and Gated GELU.

<!-- dataset: Wikipedia and BooksCorpus -->
<!-- fine-tunable: true -->
<!-- format: saved_model_2 -->
<!-- language: en -->
<!-- module-type: text-embedding -->
<!-- network-architecture: Transformer -->

## Overview

*BERT with Talking-Heads Attention and Gated GELU position-wise feed-forward
networks* is a Transformer Encoder for text that modifies the
[BERT architecture](https://arxiv.org/abs/1810.04805)
by using talking-heads attention (instead of multi-head attention) and
by using a gated linear unit with GELU activation as the first layer
of the position-wise feed-forward networks (instead of an ordinary dense layer)
as proposed by

  * Noam Shazeer, Zhenzhong Lan, Youlong Cheng, Nan Ding, Le Hou:
    ["Talking-Heads Attention"](https://arxiv.org/abs/2003.02436), 2020.
  * Noam Shazeer:
    ["GLU Variants Improve Transformer"](https://arxiv.org/abs/2002.05202),
    2020.

This model was implemented with code from the [TensorFlow Models
repository](https://github.com/tensorflow/models/tree/master/official/nlp)
using the `EncoderScaffold` with `GatedFeedforward` and `TalkingHeadsAttention`.
It uses L=24 hidden layers (i.e., Transformer blocks),
a hidden size of H=1024 between the Transformer blocks
and 4H inside the position-wise feed-forward networks (like the original BERT,
so that the gated unit increases the total number of weights).
The number of attention heads is 16, identically for the
separate "heads dimensions" `h`, `h_k` and `h_v` from the Talking Heads paper.

This model has been pre-trained for English
on the Wikipedia and BooksCorpus.
Text inputs have been normalized the "uncased" way, meaning that the text has
been lower-cased before tokenization into word pieces, and any accent markers
have been stripped.

All parameters in the module are trainable, and fine-tuning all parameters is
the recommended practice.


## Usage

This model expects three int32 Tensors as input: numeric token ids,
an input mask to hold out padding tokens,
and input types to mark different segments within one input (if any).
The separate **preprocessing model** at
[https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1](https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1)
transforms plain text inputs into this format.

```python
seq_length = 128  # Your choice here.
inputs = dict(
    input_word_ids=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
    input_mask=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
    input_type_ids=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32))
encoder = hub.KerasLayer(
    "https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_large/1",
    trainable=True)
outputs = encoder(inputs)
pooled_output = outputs["pooled_output"]      # [batch_size, 1024].
sequence_output = outputs["sequence_output"]  # [batch_size, seq_length, 1024].
```

The encoder's outputs are the `pooled_output` to represents each input sequence
as a whole, and the `sequence_output` to represent each input token in context.

For advanced uses, the intermediate activations of all L=24
Transformer blocks (hidden layers) are returned as a Python list:
`outputs["encoder_outputs"][i]` is a Tensor
of shape `[batch_size, seq_length, 1024]`
with the outputs of the i-th Transformer block, for `0 <= i < L`.
The last value of the list is equal to `sequence_output`.


## Changelog

### Version 1

  * Initial release.
