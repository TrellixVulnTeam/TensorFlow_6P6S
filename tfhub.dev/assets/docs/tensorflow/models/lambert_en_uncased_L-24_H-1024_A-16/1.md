# Module tensorflow/&zwnj;lambert_en_uncased_L-24_H-1024_A-16/1
Lambert: BERT trained with LAMB and techniques from RoBERTa.

<!-- dataset: Wikipedia and BooksCorpus -->
<!-- fine-tunable: true -->
<!-- format: saved_model_2 -->
<!-- language: en -->
<!-- module-type: text-embedding -->
<!-- network-architecture: Transformer -->

## Overview

Lambert is a Transformer Encoder for text that uses
the [BERT architecture](https://arxiv.org/abs/1810.04805),
trained with the [LAMB optimizer](https://arxiv.org/abs/1904.00962)
and some of the training techniques introduced by
[RoBERTa](https://arxiv.org/abs/1907.11692).
Like BERT, this model has been pre-trained for English
on the Wikipedia and BooksCorpus.
Text inputs have been normalized the "uncased" way, meaning that the text has
been lower-cased before tokenization into word pieces, and any accent markers
have been stripped.

Compared to the original BERT training process, Lambert achieves a significantly
better performance on the pre-training and downstream tasks, with no change
to the network architecture or the dataset used.
However, fine-tuning for a downstream tasks requires a careful optimizer
set-up, see below.

This Lambert model uses the implementation of BERT from the
TensorFlow Models repository on GitHub at
[tensorflow/models/official/nlp/bert](https://github.com/tensorflow/models/tree/master/official/nlp/bert).
It uses L=24 hidden layers (i.e., Transformer blocks),
a hidden size of H=1024,
and A=16 attention heads.


## Usage

This model expects three int32 Tensors as input: numeric token ids,
an input mask to hold out padding tokens,
and input types to mark different segments within one input (if any).
The separate **preprocessing model** at
[https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1](https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1)
transforms plain text inputs into this format.

```python
seq_length = 128  # Your choice here.
inputs = dict(
    input_word_ids=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
    input_mask=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32),
    input_type_ids=tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32))
encoder = hub.KerasLayer(
    "https://tfhub.dev/tensorflow/lambert_en_uncased_L-24_H-1024_A-16/1",
    trainable=True)
outputs = encoder(inputs)
pooled_output = outputs["pooled_output"]      # [batch_size, 1024].
sequence_output = outputs["sequence_output"]  # [batch_size, seq_length, 1024].
```

The encoder's outputs are the `pooled_output` to represents each input sequence
as a whole, and the `sequence_output` to represent each input token in context.

For advanced uses, the intermediate activations of all L=24
Transformer blocks (hidden layers) are returned as a Python list:
`outputs["encoder_outputs"][i]` is a Tensor
of shape `[batch_size, seq_length, 1024]`
with the outputs of the i-th Transformer block, for `0 <= i < L`.
The last value of the list is equal to `sequence_output`.


## Training details

### Pre-training

Compared to the original BERT pre-training process, the key differences are
as follows:

  1. We use the LAMB optimizer for 200,000 steps with a peak learning rate
     of `2.5e-3`, `beta_2=0.99` and a batch size of 8192.
  2. We duplicate the training dataset 100 times, each time with different
     token masks.
  3. We use the FULL-SENTENCES (cross-document) method discussed in the
     [RoBERTa paper](https://arxiv.org/abs/1907.11692)
     to prepare training sequences of up to 512 tokens.
  4. Like RoBERTa, we do not use the next sentence prediction task.

Lambert achieves significantly better pretraining and fine-tuning performance
compared to the original BERT-large model of the same size:

  1. In pretraining, the masked language model accuracy reached 77.4%,
     as opposed to 74.2% by the original BERT-large model.
  2. Finetuning on SQuAD v1.1 yields an F-1 score of 94.1%,
     as opposed to 92.5% by the original BERT-large model.
  3. Finetuning on SQuAD v2.0 yields an F-1 score of 88.0%,
     as opposed to 84.5% by the original BERT-large model.
  4. Finetuning on GLUE/MNLI-matched yields an accuracy of 88.7%,
     as opposed to 86.8% by the original BERT-large model.

### Fine-tuning

All parameters in the module are trainable, and fine-tuning all parameters is
the recommended practice.

We tested the model by fine-tuning it on two tasks: SQuAD and GLUE/MNLI-matched. We found that:

 1. For SQuAD v1.1 and SQuAD v2.0, a lower learning rate is preferred
    compared to the original BERT. In particular, we found that using the
    AdamWeightdecay optimizer with a peak learning rate of 3.0e-5
    (with learning rate warmup followed by linear decay) yields the expected
    F-1 scores consistently.

 2. For GLUE/MNLI-matched, we found that the fine-tuning is brittle — using
    the AdamWeightdecay optimizer, the model cannot converge to a desired
    local optimum. Instead, the classification accuracy on the dev-set is
    always around 0.33 (like random guessing).
    Using the LAMB optimizer with a peak learning rate
    of 3.0e-5 (with learning rate warmup followed by linear decay), the model
    achieved an accuracy of 0.89 in 2 out of 3 runs. The other run also failed
    with an accuracy of 0.33. Note that with LAMB, we also set

      * `weight_decay_rate=0.01`,
      * `exclude_from_layer_adaptation=[“LayerNorm”, “layer_norm“, “bias“]`,
      * `exclude_from_weight_decay=[“LayerNorm”, “layer_norm“, “bias“]`.

    You can adopt this LAMB setting for fine-tuning on GLUE tasks.
    Just keep in mind that sometimes the training does not converge.


## Changelog

### Version 1

  * Initial release.
