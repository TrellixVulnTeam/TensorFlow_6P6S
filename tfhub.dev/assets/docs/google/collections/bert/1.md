# Collection google/bert/1

Bidirectional Encoder Representations from Transformers (BERT).

<!-- dataset: Wikipedia and BooksCorpus -->
<!-- module-type: text-embedding -->
<!-- network-architecture: Transformer -->
<!-- language: en -->

## Overview

TF Hub offers BERT models in the
[SavedModel format for TF2](https://www.tensorflow.org/hub/tf2_saved_model),
and also in the older, now deprecated
[Hub module format for TF1](https://www.tensorflow.org/hub/tf1_hub_module).

## TF2 SavedModel collection

These models use the implementation of BERT [1] from the TensorFlow Models
repository on GitHub at
[tensorflow/models/official/nlp/bert](https://github.com/tensorflow/models/tree/master/official/nlp/bert).

Plain text input can be fed to these models via the separate preprocessing
model referenced on each model page.

All parameters in the models are trainable, and fine-tuning all parameters is
the recommended practice.

|            |
|------------|
| [tensorflow/bert_en_uncased_L-24_H-1024_A-16](https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16) |
| [tensorflow/bert_en_wwm_cased_L-24_H-1024_A-16](https://tfhub.dev/tensorflow/bert_en_wwm_cased_L-24_H-1024_A-16) |
| [tensorflow/bert_en_uncased_L-12_H-768_A-12](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12) |
| [tensorflow/bert_en_wwm_uncased_L-24_H-1024_A-16](https://tfhub.dev/tensorflow/bert_en_wwm_uncased_L-24_H-1024_A-16) |
| [tensorflow/bert_en_cased_L-24_H-1024_A-16](https://tfhub.dev/tensorflow/bert_en_cased_L-24_H-1024_A-16) |
| [tensorflow/bert_en_cased_L-12_H-768_A-12](https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12) |
| [tensorflow/bert_zh_L-12_H-768_A-12](https://tfhub.dev/tensorflow/bert_zh_L-12_H-768_A-12) |
| [tensorflow/bert_multi_cased_L-12_H-768_A-12](https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12) |

Additionally, there are the "Small BERT" models (English, uncased) described in
the paper "Well-Read Students Learn Better" [2] built from the same codebase.

|   |H=128|H=256|H=512|H=768|
|---|:---:|:---:|:---:|:---:|
| **L=2**  | [2/128](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2)  | [2/256](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4)  | [2/512](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8)   | [2/768](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12)   |
| **L=4**  | [4/128](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2)  | [4/256](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4)  | [4/512](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8)   | [4/768](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12)   |
| **L=6**  | [6/128](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2)  | [6/256](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4)  | [6/512](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8)   | [6/768](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12)   |
| **L=8**  | [8/128](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2)  | [8/256](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4)  | [8/512](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8)   | [8/768](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12)   |
| **L=10** | [10/128](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2)| [10/256](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4)| [10/512](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8) | [10/768](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12) |
| **L=12** | [12/128](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2)| [12/256](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4)| [12/512](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8) | [12/768](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12) |


## TF1 hub.Modules collection (deprecated)

These models use the original BERT implementation from
[github.com/google-research/bert](https://github.com/google-research/bert)
described in the paper "BERT: Pre-training of Deep Bidirectional Transformers
for Language Understanding" [1].

The models assume pre-processed inputs from the BERT repository
(https://github.com/google-research/bert)

All parameters in the module are trainable, and fine-tuning all parameters is
the recommended practice.

|            |
|------------|
| [google/bert_cased_L-12_H-768_A-12](https://tfhub.dev/google/bert_cased_L-12_H-768_A-12) |
| [google/bert_cased_L-24_H-1024_A-16](https://tfhub.dev/google/bert_cased_L-24_H-1024_A-16) |
| [google/bert_chinese_L-12_H-768_A-12](https://tfhub.dev/google/bert_chinese_L-12_H-768_A-12) |
| [google/bert_multi_cased_L-12_H-768_A-12](https://tfhub.dev/google/bert_multi_cased_L-12_H-768_A-12) |
| [google/bert_uncased_L-12_H-768_A-12](https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12) |
| [google/bert_uncased_L-24_H-1024_A-16](https://tfhub.dev/google/bert_uncased_L-24_H-1024_A-16) |

Additionally, there are the "Small BERT" models (English, uncased) described in
the paper "Well-Read Students Learn Better" [2] built from the same codebase.

|   |H=128|H=256|H=512|H=768|
|---|:---:|:---:|:---:|:---:|
| **L=2**  | [2/128](https://tfhub.dev/google/small_bert/bert_uncased_L-2_H-128_A-2)  | [2/256](https://tfhub.dev/google/small_bert/bert_uncased_L-2_H-256_A-4)  | [2/512](https://tfhub.dev/google/small_bert/bert_uncased_L-2_H-512_A-8)   | [2/768](https://tfhub.dev/google/small_bert/bert_uncased_L-2_H-768_A-12)   |
| **L=4**  | [4/128](https://tfhub.dev/google/small_bert/bert_uncased_L-4_H-128_A-2)  | [4/256](https://tfhub.dev/google/small_bert/bert_uncased_L-4_H-256_A-4)  | [4/512](https://tfhub.dev/google/small_bert/bert_uncased_L-4_H-512_A-8)   | [4/768](https://tfhub.dev/google/small_bert/bert_uncased_L-4_H-768_A-12)   |
| **L=6**  | [6/128](https://tfhub.dev/google/small_bert/bert_uncased_L-6_H-128_A-2)  | [6/256](https://tfhub.dev/google/small_bert/bert_uncased_L-6_H-256_A-4)  | [6/512](https://tfhub.dev/google/small_bert/bert_uncased_L-6_H-512_A-8)   | [6/768](https://tfhub.dev/google/small_bert/bert_uncased_L-6_H-768_A-12)   |
| **L=8**  | [8/128](https://tfhub.dev/google/small_bert/bert_uncased_L-8_H-128_A-2)  | [8/256](https://tfhub.dev/google/small_bert/bert_uncased_L-8_H-256_A-4)  | [8/512](https://tfhub.dev/google/small_bert/bert_uncased_L-8_H-512_A-8)   | [8/768](https://tfhub.dev/google/small_bert/bert_uncased_L-8_H-768_A-12)   |
| **L=10** | [10/128](https://tfhub.dev/google/small_bert/bert_uncased_L-10_H-128_A-2)| [10/256](https://tfhub.dev/google/small_bert/bert_uncased_L-10_H-256_A-4)| [10/512](https://tfhub.dev/google/small_bert/bert_uncased_L-10_H-512_A-8) | [10/768](https://tfhub.dev/google/small_bert/bert_uncased_L-10_H-768_A-12) |
| **L=12** | [12/128](https://tfhub.dev/google/small_bert/bert_uncased_L-12_H-128_A-2)| [12/256](https://tfhub.dev/google/small_bert/bert_uncased_L-12_H-256_A-4)| [12/512](https://tfhub.dev/google/small_bert/bert_uncased_L-12_H-512_A-8) | [12/768](https://tfhub.dev/google/small_bert/bert_uncased_L-12_H-768_A-12) |

## References

[1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. [BERT:
Pre-training of Deep Bidirectional Transformers for Language
Understanding](https://arxiv.org/abs/1810.04805). arXiv preprint
arXiv:1810.04805, 2018.

[2] Iulia Turc, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. [Well-Read
Students Learn Better: On the Importance of Pre-training Compact
Models](https://arxiv.org/abs/1908.08962). arXiv preprint arXiv:1908.08962,
2019.
